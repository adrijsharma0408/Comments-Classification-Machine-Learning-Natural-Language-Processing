{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import all the necessary python libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"72842684a5e69537516152d82ba945817c76b92a","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import regex as re\n","import pickle\n","import nltk\n","import time\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from imblearn.ensemble import EasyEnsembleClassifier\n","from sklearn.linear_model import Ridge\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.model_selection import cross_val_score"]},{"cell_type":"markdown","metadata":{},"source":["# Start the code execution timer, Read the train and test .csv files "]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"3c2134c2bdca849a678ce6d4b9cbbf736a7f0f1c","trusted":true},"outputs":[],"source":["Start = time.time()\n","train = pd.read_csv('train.csv').fillna(' ')\n","test = pd.read_csv('test.csv').fillna(' ')"]},{"cell_type":"markdown","metadata":{},"source":["# PRE-PROCESSING"]},{"cell_type":"markdown","metadata":{},"source":["## Lemmatization of Training & Test Comments"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","lemmatizer = WordNetLemmatizer()\n","w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","stopWords = stopwords.words('english')\n","\n","def lemmatize_text(text):\n","    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n","\n","train_text = train[\"text\"]\n","test_text = test[\"text\"]\n","train_text = train_text.apply(lemmatize_text) \n","test_text = test_text.apply(lemmatize_text)\n","train_text = train_text.apply(lambda x: ' '.join(([word for word in x]))) # Join back the lemmztized words to form complete sentences.\n","test_text = test_text.apply(lambda x: ' '.join(([word for word in x])))\n","train[\"text\"] = train_text\n","test[\"text\"] = test_text\n","del train_text, test_text\n","\n","# train_text = train_text.apply(lambda x: ' '.join(([word for word in x if word not in (stopWords)])))  \n","# Instead of removing the stop words here, we take care of them during tokenization, vectorization of words and sentences."]},{"cell_type":"markdown","metadata":{},"source":["## Manual Text Clean-up & Pre-processing"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["temporary_training_data = []\n","temporary_testing_data = []\n","train_comments_list = train[\"text\"].tolist()\n","test_comments_list = test[\"text\"].tolist()\n","\n","sensibleWords = {\n","    \":')\": \" sad \",\n","    \":-(\": \" sad \",\n","    \":(\": \" sad \",\n","    \":s\": \" sad \",\n","    \":-s\": \" sad \",\n","    \":-(\": \" frown \",\n","    \":(\": \" frown \",\n","    \":s\": \" frown \",\n","    \":-s\": \" frown \",\n","    \":/\": \" bad \",\n","    \":&gt;\": \" sad \",\n","    \":')\": \" sad \",\n","    \"&lt;3\": \" heart \",\n","    \":/\": \" worry \",\n","    \":&gt;\": \" angry \",\n","    \"yay!\": \" good \",\n","    \"yay\": \" good \",\n","    \"yaay\": \" good \",\n","    \"yaaay\": \" good \",\n","    \"yaaaay\": \" good \",\n","    \"yaaaaay\": \" good \",\n","    \"m\": \"am\",\n","    \"r\": \"are\",\n","    \"u\": \"you\",\n","    \"haha\": \"ha\",\n","    \"hahaha\": \"ha\",\n","    \"don't\": \"do not\",\n","    \"haven't\": \"have not\",\n","    \"hadn't\": \"had not\",\n","    \"won't\": \"will not\",\n","    \"wouldn't\": \"would not\",\n","    \"can't\": \"can not\",\n","    \"cannot\": \"can not\",\n","    \"i'm\": \"i am\",\n","    \"m\": \"am\",\n","    \"i'll\" : \"i will\",\n","    \"its\" : \"it is\",\n","    \"it's\" : \"it is\",\n","    \"'s\" : \" is\",\n","    \"that's\" : \"that is\",\n","    \"weren't\" : \"were not\",\n","    \"doesn't\": \"does not\",\n","    \"didn't\": \"did not\",\n","    \"hasn't\": \"has not\",\n","    \":d\": \" smile \",\n","    \":p\": \" smile \",\n","    \":dd\": \" smile \",\n","    \"8)\": \" smile \",\n","    \":-)\": \" smile \",\n","    \":)\": \" smile \",\n","    \";)\": \" smile \",\n","    \"(-:\": \" smile \",\n","    \"(:\": \" smile \",\n","    r\"\\br\\b\": \"are\",\n","    r\"\\bu\\b\": \"you\",\n","    r\"\\bhaha\\b\": \"ha\",\n","    r\"\\bhahaha\\b\": \"ha\",\n","    r\"\\bdon't\\b\": \"do not\",\n","    r\"\\bdoesn't\\b\": \"does not\",\n","    r\"\\bdidn't\\b\": \"did not\",\n","    r\"\\bhasn't\\b\": \"has not\",\n","    r\"\\bhaven't\\b\": \"have not\",\n","    r\"\\bhadn't\\b\": \"had not\",\n","    r\"\\bwon't\\b\": \"will not\",\n","    r\"\\bwouldn't\\b\": \"would not\",\n","    r\"\\bcan't\\b\": \"can not\",\n","    r\"\\bcannot\\b\": \"can not\",\n","    r\"\\bi'm\\b\": \"i am\",\n","}\n","\n","nonsenseWords = [word for word in sensibleWords.keys()] \n","# A set of all words which need to be furthur defined to make them meaningful and understandable by the machine.\n","\n","for comment in train_comments_list:\n","    words = str(comment).split()\n","    sentense = \"\"\n","    for word in words:\n","        # word = str(word).lower()\n","        # Lower case and upper case words carry different meanings.\n","        if word[:4] == 'http' or word[:3] == 'www': # If we encounter some links in the comments section, \n","            continue                                # then we ignore the unimportant words in the URL of the link and pay attension towards the valuable text.\n","        if word in nonsenseWords:\n","            word = sensibleWords[word]\n","        sentense += word + \" \"  # Make a sentence out all the cleaned up and relevant words from the given comment.\n","    temporary_training_data.append(sentense)\n","\n","for comment in test_comments_list:\n","    words = str(comment).split()\n","    sentense = \"\"\n","    for word in words:\n","        # word = str(word).lower() \n","        # Lower case and upper case words carry different meanings.\n","        if word[:4] == 'http' or word[:3] == 'www': # If we encounter some links in the comments section, \n","            continue                                # then we ignore the unimportant words in the URL of the link and pay attension towards the valuable text.\n","        if word in nonsenseWords:\n","            word = sensibleWords[word]\n","        sentense += word + \" \"  # Make a sentence out all the cleaned up and relevant words from the given comment.\n","    temporary_testing_data.append(sentense)\n","\n","# Remove all the numbers and punctuation marks apart from '?' and '!' from the training and test comments. \n","for i, c in enumerate(temporary_training_data):\n","    temporary_training_data[i] = re.sub('[^a-zA-Z ?!]+', '', temporary_training_data[i])\n","\n","for i, c in enumerate(temporary_testing_data):\n","    temporary_testing_data[i] = re.sub('[^a-zA-Z ?!]+', '', temporary_testing_data[i]) \n","    \n","train[\"text\"] = temporary_training_data\n","test[\"text\"] = temporary_testing_data\n","del temporary_training_data, temporary_testing_data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_text = train[\"text\"]\n","test_text = test[\"text\"]\n","complete_text = pd.concat([train[\"text\"], test[\"text\"]]) \n","# We concatenate the preprocessed training and test comments so that we get a better fit for the tfidf model."]},{"cell_type":"markdown","metadata":{},"source":["# EMBEDDING & VECTORIZATION"]},{"cell_type":"markdown","metadata":{},"source":["## Word Embedding - Vectorization of Words using TF-IDF (Analyser = 'word')"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"dc4896cb2ea4a9bff6f127e41c31ce028fa11b15","trusted":true},"outputs":[],"source":["word_vectorizer = TfidfVectorizer(\n","    sublinear_tf = True,  # It seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence. \n","    # Accordingly, there has been considerable research into variants of term frequency that go beyond counting the number of occurrences of a term. \n","    # A common modification is to use instead the logarithm of the term frequency, which assigns a weight. 1 + log(tf)\n","    strip_accents = 'unicode', # Remove accents and perform other character normalization during the preprocessing step. \n","                               # ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. \n","                               # ‘unicode’ is a slightly slower method that works on any characters.\n","    analyzer = 'word',  # Whether the feature should be made of word or character.\n","    token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n","    lowercase = False,  # Do not convert the uppercase letters into lowercase because they carry significance.\n","    stop_words = 'english',  # Remove all the stop words of english \n","    # ngram is the set of n words together.\n","    ngram_range = (1, 2),  # We consider set of 1 or 2 words together for tokenization.\n","    min_df = 2,\n","    max_df = 0.5,\n","    norm = 'l2',\n","    max_features = 30000\n",") #lowercase = true : convert all characters into lower case before tokenzing\n","word_vectorizer.fit(complete_text) # Apply tfidf fitting on the whole preprocessed text data so that we schieve a better fitted model.\n","train_word_features = word_vectorizer.transform(train_text)\n","test_word_features = word_vectorizer.transform(test_text)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# word_vectorizer = TfidfVectorizer(\n","#     sublinear_tf = True,\n","#     strip_accents = 'unicode',\n","#     tokenizer = lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n","#     analyzer = 'word',\n","#     min_df = 5,\n","#     norm = 'l2',\n","#     lowercase = True,\n","#     ngram_range = (1, 1),\n","#     max_features = 60000\n","# )\n","# word_vectorizer.fit(complete_text)\n","# train_word_features = word_vectorizer.transform(train_text)\n","# test_word_features = word_vectorizer.transform(test_text)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# word_vectorizer = TfidfVectorizer(\n","#     sublinear_tf = True,\n","#     strip_accents = 'unicode', # Remove accents and perform other character normalization during the preprocessing step. \n","                                 # ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. \n","                                 # ‘unicode’ is a slightly slower method that works on any characters.\n","#     tokenizer = None,\n","#     analyzer = 'word',\n","#     token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n","#     min_df = 5,\n","#     norm = 'l2',\n","#     lowercase = True,\n","#     ngram_range = (1, 1),\n","#     max_features = 60000\n","# )\n","# word_vectorizer.fit(complete_text)\n","# train_word_features = word_vectorizer.transform(train_text)\n","# test_word_features = word_vectorizer.transform(test_text)"]},{"cell_type":"markdown","metadata":{},"source":["## Character Embedding - Vectorization of Individual Characters using TF-IDF (Analyser = 'char')"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"38e3d44b83cc567567587276f65ceec7d9c70bdd","trusted":true},"outputs":[],"source":["char_vectorizer = TfidfVectorizer (\n","    sublinear_tf = True,\n","    strip_accents = 'unicode', # Remove accents and perform other character normalization during the preprocessing step. \n","                               # ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. \n","                               # ‘unicode’ is a slightly slower method that works on any characters.\n","    analyzer = 'char',\n","    ngram_range = (2, 6),  # ngram is the set of n words together.\n","    min_df = 2, \n","    max_df = 0.5,\n","    max_features = 20000\n",")\n","char_vectorizer.fit(complete_text) # We fit on complete training + test data so as to achieve a better fit.\n","train_char_features = char_vectorizer.transform(train_text)\n","test_char_features = char_vectorizer.transform(test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# char_vectorizer = TfidfVectorizer (\n","#     sublinear_tf = True,\n","#     strip_accents = 'unicode',\n","#     analyzer = 'char',\n","#     ngram_range = (2, 4), \n","#     min_df = 5, \n","#     max_df = 0.5,\n","#     max_features = 25000\n","# )\n","# char_vectorizer.fit(complete_text)\n","# train_char_features = char_vectorizer.transform(train_text)\n","# test_char_features = char_vectorizer.transform(test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# char_vectorizer = TfidfVectorizer (\n","#     sublinear_tf = True,\n","#     strip_accents = 'unicode',\n","#     analyzer = 'char',\n","#     ngram_range = (2, 6), \n","#     min_df = 2, \n","#     max_df = 0.5,\n","#     max_features = 60000\n","# )\n","# char_vectorizer.fit(complete_text)\n","# train_char_features = char_vectorizer.transform(train_text)\n","# test_char_features = char_vectorizer.transform(test_text)"]},{"cell_type":"markdown","metadata":{},"source":["## Merge the character and word vectorization results horizontally together to get the actual training and test features"]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"756c6863cfb59703b2b036b182f5210f46dfa28f","trusted":true},"outputs":[],"source":["train_features = hstack([train_char_features, train_word_features]) # Horizontally merging the training and test features. \n","test_features = hstack([test_char_features, test_word_features])"]},{"cell_type":"markdown","metadata":{},"source":["# Categorisation of Test Data & Naive Bayes Feature Equation"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["categories = ['harsh', 'extremely_harsh', 'vulgar', 'threatening', 'disrespect', 'targeted_hate']\n","temp = train_features.tocsr()\n","\n","# Naive Bayes Feature Equation\n","def pr(y_i, y): \n","    p = temp[y==y_i].sum(0)\n","    return (p + 1) / ((y == y_i).sum() + 1)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# CLASSIFICATION MODELS"]},{"cell_type":"markdown","metadata":{},"source":["## Pickle File: \n","1. Model once trained can be stored in pickle(.pckl) file.\n","2. Model can be loaded from this file to make predictions without re-training.\n","3. This saves us training time making the code more efficient.\n","4. The application of pickle file is to serialize your machine learning algorithms and save the serialized format."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Voting Classifier: Trains on 4 different classification models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cross_validation_scores = []\n","ratio_values = []\n","model_storage = open('model_storage.pckl', 'wb') # Pickle file for storing the trained models.\n","\n","# A Voting Classifier is a machine learning model that trains on an ensemble of numerous models:\n","# 1. Logistic Regression.\n","# 2. Random Forest Classifier.\n","# 3. Easy Ensemble Classifier with logistic regression as the base eliminator.\n","# 4. Easy Ensemble Classifier with SGDC Classifier as the base eliminator.\n","# The voting classifier predicts an output (class) based on the highest probability of the varioes models of chosen class as the output.\n","\n","for category in categories:\n","    train_target = train[category]\n","    train_target_values = train_target.values\n","    r = np.log(pr(1, train_target_values) / pr(0, train_target_values))\n","    x_nb = train_features.multiply(r)\n","    rfc = RandomForestClassifier(max_features = 999, max_depth = 100, min_samples_split = 10, criterion = 'gini', n_estimators = 120, min_weight_fraction_leaf = 0.0, max_leaf_nodes = None)\n","    lr = LogisticRegression(max_iter = 499, dual = False, C = 2)\n","    eec_lr = EasyEnsembleClassifier(base_estimator = LogisticRegression(solver = 'sag', max_iter = 450, C = 2)) # sag solver proved to be faster and more accurate than liblinear solver and the default solver. \n","    eec_sgdc = EasyEnsembleClassifier(base_estimator = SGDClassifier(max_iter = 165, alpha = 0.0002, penalty = \"l2\", loss = 'modified_huber'))\n","    vote = VotingClassifier(voting = 'soft', estimators = [('lr', eec_lr), ('sgd', eec_sgdc), ('lr1', lr), ('rdf', rfc)], weights = [0.9, 1.35, 0.65, 0.8])\n","    vote.fit(x_nb, train_target_values)\n","    ratio_values.append(r)  \n","    pickle.dump(vote, model_storage)  # Dump the model into the pickle file after fitting it on the training data.\n","    cv_score = np.mean(cross_val_score(vote, x_nb, train_target, cv = 5, scoring = 'roc_auc'))\n","    cross_validation_scores.append(cv_score)\n","    #print('Cross Validation score for class {} is {}'.format(category, cv_score))\n","\n","print('Cross Validation score is {}'.format(np.mean(cross_validation_scores))) # Total cross validation score.\n","model_storage.close()"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Ridge Classification Model"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"2acde6500d7796d4d833afd8034308f254ecb5c3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation score is 0.9834960337914218\n"]}],"source":["# cross_validation_scores = []\n","# model_storage = open('model_storage.pckl', 'wb')\n","\n","# for category in categories:\n","#     train_target = train[category]\n","#     #rigde = Ridge(alpha = 29, copy_X = True, fit_intercept = True, solver = 'sag', max_iter = 500, normalize = False, random_state = 0, tol = 0.0025)\n","#     ridgeClassifier = Ridge(solver = 'sag', max_iter = 150, fit_intercept = True, tol = 0.0025, alpha = 29, copy_X = True, random_state = 0)\n","#     cv_score = np.mean(cross_val_score(ridgeClassifier, train_features, train_target, cv = 3, scoring = 'roc_auc'))\n","#     cross_validation_scores.append(cv_score)\n","#     #print('Cross Validation score for class {} is {}'.format(category, cv_score))\n","#     ridgeClassifier.fit(train_features, train_target)\n","#     pickle.dump(ridgeClassifier, model_storage)\n","\n","# model_storage.close()\n","# print('Cross Validation score is {}'.format(np.mean(cross_validation_scores)))"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Random Forest Classification Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cross_validation_scores = []\n","# result = pd.DataFrame.from_dict({'id': test['id']})\n","# model_storage = open('model_storage.pckl', 'wb')\n","\n","# for category in categories:\n","#     train_target = train[category]\n","#     rfc = RandomForestClassifier(max_features = 999, max_depth = 100, min_samples_split = 10, criterion = 'gini', n_estimators = 120, min_weight_fraction_leaf = 0.0, max_leaf_nodes = None)\n","#     cv_score = np.mean(cross_val_score(rfc, train_features, train_target, cv=3, scoring='roc_auc'))\n","#     cross_validation_scores.append(cv_score)\n","#     #print('CV score for class {} is {}'.format(category, cv_score))\n","#     rfc.fit(train_features, train_target)\n","#     pickle.dump(rfc, model_storage)\n","\n","# model_storage.close()\n","# print('Cross Validation score is {}'.format(np.mean(cross_validation_scores)))"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Logistic Regression CV along with Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from imblearn.under_sampling import NearMiss  # Under Sampling\n","# nm = NearMiss(random state = 100)\n","\n","# # from imblearn.over_sampling import SMOTE  # Over Sampling\n","# # smk = SMOTE(random_state = 12)\n","\n","# from sklearn.linear_model import LogisticRegressionCV\n","# lr = LogisticRegressionCV(solver = 'liblinear', n_jobs = -1) # liblinear solver proved to be the best\n","\n","# for category in categories:\n","#     Y_train = train_df[category].to_numpy().astype(np.float64)\n","#     X_train_balanced, Y_train_balanced = nm.fit_resample(X_train, Y_train)\n","#     lr.fit(X_train_balanced, Y_train_balanced)\n","#     pickle.dump(lr, model_storage)\n","\n","# model_storage.close()"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions on the test data (Separately for all categories) & output .csv preparation "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0523dbf4d2b57de7c4b6ead1520fa563f29d63b3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Time of execution: 371.9367505431175 minutes\n"]}],"source":["Output = pd.DataFrame.from_dict({'id': test['id']})\n","models = []\n","\n","with open(\"model_storage.pckl\", \"rb\") as file:\n","    while True:\n","        try:\n","            models.append(pickle.load(file)) # Collect the models that have been saved in the pickle file and make the probability predictions accordingly.\n","        except EOFError:\n","            break\n","\n","# We need to use .predict_proba(test data) for all other models except Ridge Classifier.\n","# The ridge classifier works with simple .predict(test data) function.\n","i=0  \n","for category in categories:\n","    train_target = train[category]\n","    #Output[category] = models[i].predict(test_features)\n","    Output[category] = models[i].predict_proba(test_features.multiply(ratio_values[i]))[:, 1] # Predictions made by the trained models on the test data.\n","    i=i+1\n","\n","Output.to_csv('FinalSubmission.csv', index = False) # Final kaggle submission .csv file.\n","\n","End = time.time()\n","print('Time of execution: {} minutes'.format((End - Start) / 60)) # Print the code execution time."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":1}
